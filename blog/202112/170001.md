---
title: 论文《Wide & Deep Learning for Recommender Systems》阅读
---

# 论文《Wide & Deep Learning for Recommender Systems》阅读

<script type="text/javascript" src="/include/head.js"></script>

本文提出了Wide & Deep模型，使得推荐系统同时具备记忆和泛化两种能力。

## 记忆能力与泛化能力

推荐系统的一个挑战在于如何同时获得记忆能力memorization和泛化能力generalization。记忆能力memorization是指学习历史数据频繁出现的特征组合；泛化能力generalization是指根据相关性的传递性，探索历史数据中没有或很少出现的特征组合。记忆能力memorization使得推荐系统更关注用户兴趣，泛化能力generalization使得推荐系统更具多样性。

Wide模型指广义线性模型（例如Logistic回归），一般采用01二值特征。通过叉积（也就是二值特征两两组合）获得记忆能力，通过使用粒度更小的特征获得泛化能力（这需要更过人工特征工程），训练数据中没有出现的特征对没办法泛化。

Deep模型指基于Embedding的模型（例如因子分解机、深度神经网络），通过学习Embedding向量进行泛化，减少人工特征工程。但是如果某个特征在历史数据中的共现特征很少，通常无法学习有效的Embedding，并且会高估特征之间的相关性（Embedding向量的内积通常不为0，但是这种情况下真实的特征组合应该是0）。

Wide & Deep通过联合训练线性模型和神经网络，同时提高记忆能力和泛化能力。

## Wide & Deep模型

### Wide部分

Wide部分是广义线性模型：

$$y = w^Tx+b$$

其中，$y$是输出预测，$x$是输入特征，$w$是模型参数，$b$是偏置；

输入包括原始特征和转换特征（主要是叉积，即特征交叉），特征交叉学习二值特征共现，并为广义线性模型添加了非线性。

### Deep部分

Deep部分是前馈神经网络，先将高纬one-hot特征用随机初始化的转换矩阵转换为低维Embedding特征，然后送入隐藏层：

$$a^{(l+1)} = f(W^{(l)}a^{l}+b^{(l)})$$

其中，$l$是层数，$f$是激活函数（通常是ReLU），$a^{l}$、$b^{(l)}$、$W^{(l)}$是第$l$层的输入、偏置、权重。

### 两部分联合训练

将两个部分的输出拼接，再过一个线性层（乘weight、加bias），最后过Logistic函数变成概率：

$$P(Y=1|X)=\sigma(W_{wide}^T[X,\Phi(X)]+W_{deep}^Ta^{l_f}+b)$$

然后选用一个Loss函数拟合。

使用带L1正则化的FTRL(Follow-the-regularized-leader)算法（一种稀疏性好，精度也不错的随机梯度下降算法）作为Wide部分的优化器（注重稀疏性，让wide的大部分系数都是0），使用AdaGrad算法作为Deep部分的优化器。

---

<script type="text/javascript" src="/include/tail.js"></script>
