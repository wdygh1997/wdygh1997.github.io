---
title: 机器学习面试题集锦之集成学习
---

# 机器学习面试题集锦之集成学习

<script type="text/javascript" src="/include/head.js"></script>

## 集成学习分为哪几种？他们有何不同？

### Boosting

串行训练，将基分类器层层叠加，每一层训练的时候，堆前一层基分类器分错的样本，给予更高的权重，最后根据各层分类器的结果的加权得到最终结果。

### Bagging

并行训练，可以类比集体决策的过程，每个基分类器都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠，由于基分类器之间存在差异性，最终做出的判断不会完全一致，在最终做决策时，每个基分类器单独做出判断，再通过投票的方式做出最后的集体决策。

### 从偏差和方差的角度理解Boosting和Bagging的不同

基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统型错误，表现在训练误差不收敛。方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。

Boosting方法是通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。

Bagging方法是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。

## 集成学习的步骤？举几个集成学习的例子？

### 集成学习的步骤

1、找到误差相互独立的基分类器

2、训练基分类器

3、合并基分类器的结果；合并的方法有voting（Bagging采用）和stacking（Boosting采用）；前者是用投票的方式，将获得最多选票的结果作为最终的结果；后者使用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加（或者用更复杂的算法融合，比如把各个基分类器的输出作为特征，使用逻辑回归作为融合模型进行最后的结果预测）作为最终的输出。

### 随机森林（Random Forest）

采用Bagging方法，基于决策树基分类器，为了让基分类器之间相互独立，将训练集分为若干子集（当训练样本数量较少时，自己之间可能有交集）。

### AdaBoost

采用Boosting方法，对分类正确的样本降低权重，对分类错误的样本升高或者保持权重不变，在最后进行模型融合的过程中，也根据错误率对基分类器进行加权融合，错误率低的分类器拥有更大的权重。

### 梯度提升决策树GBDT（Gradient Boosting Decision Tree）

采用Gradient Boosting方法，每一棵树学习的是之前所有树结论和的残差，这个残差就是一个加预测值后能得到真实值的累加量。

## 常用的基分类器是什么？随机森林中决策树作基分类器，能否替换为线性分类器或K近邻？

### 最常用基分类器

决策树，原因如下：

1、决策树可以方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整权重；

2、决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；

3、数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，更适合作为基分类器。

另外，神经网络也适合作为基分类器，主要由于神经网络模型对样本分布也较为敏感，而且还可以通过调整神经元数量、连接方式、网络层数、初始权重等方式引入随机性。

### 不能用线性分类器或K近邻替代决策树作为随机森林中的基分类器的原因

随机森林属于Bagging集成学习方法，集成后的分类器方差比基分类器的方差小，所以Bagging所采用的基分类器最好是本身对样本分布较为敏感的（即所谓不稳定的分类器），这样Bagging才有效。线性分类器或K近邻分类器都是较为稳定的分类器，本身方差就不大，所以以它们作为基分类器使用Bagging并不能在原有的基分类器的基础上获得更好的表现，甚至科恩那个因为Bagging的采样，而导致它们在训练中更难收敛，从而增大了集成分类器的偏差。

## 什么是偏差和方差？如何减小方差和偏差的角度解释Bagging和Boosting的原理？

### 偏差与方差

有监督学习中，模型的泛化误差主要来源于两个方面：偏差和方差。

偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实label之差。偏差通常是由于对学习算法做了错误的假设而导致，偏差所带来的误差通常在训练误差上就能体现出来。

方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的方差。方差通常是由于模型的复杂度相对于训练样本数m过高导致的，有方差带来的误差通常体现在测试误差相对于训练误差的增量上。

### 从减小方差和偏差的角度解释Bagging和Boosting的原理

之所以能够提高弱分类器性能是因为，Bagging降低了方差，Boosting降低了偏差。

模型复杂度过低会导致偏差较大，从而导致泛化误差较大；模型复杂度过高会导致方差较大，从而导致泛化误差较大；所以要选择合适的模型复杂度才能让泛化误差较小。

## 梯度提升决策树GBDT的基本原理？梯度提升与梯度下降区别？GBDT的优缺点？

### GBDT基本原理

梯度提升Gradient Boosting是Boosting的一种，基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。

采用CART决策树作为弱分类器的Gradient Boosting的算法称为GBDT。

### 梯度提升与梯度下降区别

梯度下降定义在参数空间中，模型的更新等价于参数的更新。

梯度提升定义在函数空间中，不需要进行参数化表示，扩展了可以使用的模型种类。

### GBDT优缺点

优点：

1、预测阶段的计算速度快，树与树之间可并行化计算；

2、在分布稠密的数据集上，泛化能力和表达能力都很好；

3、具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，并且不需要对数据进行特殊的预处理，如归一化等。

缺点：

1、在高维稀疏的数据集上，表现不如支持向量机和神经网络；

2、在处理文本分类特征问题上，相对其他模型的优势不如其在处理数值特征时明显；

3、训练阶段需要串行，只能在决策树内部采用一些局部并行手段提高训练速度。

## XGBoost和GBDT的区别与联系？

1、GBDT是机器学习算法，XGBoost是该算法的工程实现；

2、使用CART作为基分类器时，XGBoost显式地加入正则化项来控制模型地复杂度，有利于防止过拟合，提高模型泛化能力；

3、GBDT在模型训练时只使用了损失函数的一阶导数信息，XGBoost对损失函数进行二阶泰勒展开，使用了一阶和二阶导数信息；

4、传统的GBDT使用CART作为基分类器，XGBoost支持多种基分类器，比如线性分类器；

5、传统的GBDT在每轮迭代时使用全部的数据，XGBoost采用与随机森林相似的策略，对数据进行采样；

6、传统的GBDT没有对于缺失值的处理，XGBoost能自动学习出缺失值的处理策略。

---

<script type="text/javascript" src="/include/tail.js"></script>
