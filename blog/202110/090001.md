---
title: 机器学习面试题集锦之集成学习
---

# 机器学习面试题集锦之集成学习

<script type="text/javascript" src="/include/head.js"></script>

## 集成学习分为哪几种？他们有何不同？

### Boosting

串行训练，将基分类器层层叠加，每一层训练的时候，堆前一层基分类器分错的样本，给予更高的权重，最后根据各层分类器的结果的加权得到最终结果。

### Bagging

并行训练，可以类比集体决策的过程，每个基分类器都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠，由于基分类器之间存在差异性，最终做出的判断不会完全一致，在最终做决策时，每个基分类器单独做出判断，再通过投票的方式做出最后的集体决策。

### 从偏差和方差的角度理解Boosting和Bagging的不同

基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统型错误，表现在训练误差不收敛。方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。

Boosting方法是通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。

Bagging方法是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。

## 集成学习的步骤？举几个集成学习的例子？

### 集成学习的步骤

1、找到误差相互独立的基分类器

2、训练基分类器

3、合并基分类器的结果；合并的方法有voting（Bagging采用）和stacking（Boosting采用）；前者是用投票的方式，将获得最多选票的结果作为最终的结果；后者使用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加（或者用更复杂的算法融合，比如把各个基分类器的输出作为特征，使用逻辑回归作为融合模型进行最后的结果预测）作为最终的输出。

### 随机森林（Random Forest）

采用Bagging方法，基于决策树基分类器，为了让基分类器之间相互独立，将训练集分为若干子集（当训练样本数量较少时，自己之间可能有交集）。

### AdaBoost

采用Boosting方法，对分类正确的样本降低权重，对分类错误的样本升高或者保持权重不变，在最后进行模型融合的过程中，也根据错误率对基分类器进行加权融合，错误率低的分类器拥有更大的权重。

### 梯度提升决策树GBDT（Gradient Boosting Decision Tree）

采用Gradient Boosting方法，每一棵树学习的是之前所有树结论和的残差，这个残差就是一个加预测值后能得到真实值的累加量。

## 常用的基分类器是什么？随机森林中决策树作基分类器，能否替换为线性分类器或K近邻？

### 最常用基分类器

决策树，原因如下：

1、决策树可以方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整权重；

2、决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；

3、数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，更适合作为基分类器。

另外，神经网络也适合作为基分类器，主要由于神经网络模型对样本分布也较为敏感，而且还可以通过调整神经元数量、连接方式、网络层数、初始权重等方式引入随机性。

### 不能用线性分类器或K近邻替代决策树作为随机森林中的基分类器的原因

随机森林属于Bagging集成学习方法，集成后的分类器方差比基分类器的方差小，所以Bagging所采用的基分类器最好是本身对样本分布较为敏感的（即所谓不稳定的分类器），这样Bagging才有效。线性分类器或K近邻分类器都是较为稳定的分类器，本身方差就不大，所以以它们作为基分类器使用Bagging并不能在原有的基分类器的基础上获得更好的表现，甚至科恩那个因为Bagging的采样，而导致它们在训练中更难收敛，从而增大了集成分类器的偏差。

## 什么是偏差和方差？如何减小方差和偏差的角度解释Bagging和Boosting的原理？

### 偏差与方差

有监督学习中，模型的泛化误差主要来源于两个方面：偏差和方差。

偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实label之差。偏差通常是由于对学习算法做了错误的假设而导致，偏差所带来的误差通常在训练误差上就能体现出来。

方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的方差。方差通常是由于模型的复杂度相对于训练样本数m过高导致的，有方差带来的误差通常体现在测试误差相对于训练误差的增量上。

### 从减小方差和偏差的角度解释Bagging和Boosting的原理

之所以能够提高弱分类器性能是因为，Bagging降低了方差，Boosting降低了偏差。

模型复杂度过低会导致偏差较大，从而导致泛化误差较大；模型复杂度过高会导致方差较大，从而导致泛化误差较大；所以要选择合适的模型复杂度才能让泛化误差较小。

## 梯度提升决策树GBDT的基本原理



---

<script type="text/javascript" src="/include/tail.js"></script>
