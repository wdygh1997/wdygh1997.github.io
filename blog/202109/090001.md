---
title: 《深度学习推荐系统》（第2章）阅读笔记
---

# 《深度学习推荐系统》（第2章）阅读笔记

<script type="text/javascript" src="/include/head.js"></script>

《深度学习推荐系统》，作者王喆，2020年3月第1版，第2章《前深度学习时代——推荐系统的进化之路》。

## 推荐系统架构

用户信息、物品信息、场景信息，经由特征工程得到，用户特征、物品特征、场景特征；

将三种特征输入推荐系统模型，推荐系统模型由三部分组成，召回层、排序层、补充层；

推荐系统负责由候选物品库生成推荐物品列表；

模型先由离线训练，经过离线评估后上线，然后模型进行线上A/B测试并在线实时更新；

召回层（从海量数据中召回用户可能感兴趣的物品），排序层（将召回的粗筛选物品进行精排序），补充层（结合一些补充的算法策略对推荐列表进行调整）。

## 协同过滤

通过分析用户或者事物之间的相似性（所谓的协同），来预测用户可能感兴趣的内容并将此内容推荐给用户。

现有用户-物品矩阵，协同过滤有UserCF和ItemCF两种。

### UserCF

由用户向量计算用户之间的相似度，最简单的用户向量可以是用户-物品矩阵中该用户这一行的数据构成的向量；

对要推荐的用户得到与其最相近的topk个用户，然后利用用户相似度和相似用户对物品评价的加权平均来预测该该用户对物品的评分。

### ItemCF

由物品向量计算物品之间的相似度，最简单的物品向量可以是用户-物品矩阵中该物品这一列的数据构成的向量；

找出与该物品相似的topk个正反馈物品，组成相似物品集合，然后利用物品相似度和用户对相似物品的加权平均来预测用户对该物品的评分。

### 矩阵分解

一般使用截断的奇异值分解；

奇异值分解，给定矩阵$M_{m\times n}$ ，存在奇异值分解$M_{m\times n} = U_{m\times m}\Sigma_{m\times n}V_{n\times n}^T$（其中$\Sigma$为对角矩阵），实际中往往将分解后的矩阵截断使用，只取$\Sigma$前k行列方阵，即$M_{m\times n} \approx U_{m\times k}\Sigma_{k\times k}V_{n\times k}^T$；

这样，这样$U_{m\times k}$矩阵是用户特征矩阵，每个用户向量k维，$V_{n\times k}$为物品特征矩阵，每个物品向量k维。

### 消除打分偏差

加入全局偏差常数、物品偏差系数（可用物品收到的所有评分均值）、用户偏差系数（可用用户给出的所有评分均值）。

## Logistic回归

Logistic回归输出正样本的概率（比如点击率CTR），用正样本的概率排序，将成为正样本概率最高的物品推荐给用户；输入是用户的若干特征组成的特征向量。

### 特征交叉自动化

原始的Logistic回归特征之间是不交叉的，但事实上各个特征并不是独立的，所以需要特征交叉。

#### POLY2

POLY2本质上就是二阶多项式的Logistic回归，将Logistic回归中的线性函数替换成二阶函数，在套上Logistic函数。

#### FM

所有二阶项的系数矩阵是一个正定的对称矩阵（$W_{n\times n}$），可以将其分解（$W_{n\times n} = M_{n\times k}M_{n\times k}^T$），也就是说，交叉项前面的系数原本是一个常数，现在改成了两个向量的内积，这两个向量分别是交叉项两个子项的特征隐向量。

#### FFM

每个项为所有项（包括自己）创建一个特征隐向量，也就是每个项不再是一个特征隐向量，而是一组特征隐向量，当$x_1$与$x_2$交叉时，前面是系数是$x_1$特征隐向量组中与$x_2$对应的特征隐向量与$x_2$特征隐向量组中与$x_1$对应的特征隐向量的内积。

### 特征工程模型化

用GBDT生成特征向量：将训练样本输入GBDT的一个子树后，会落入一个叶子结点中，将该叶子结点置为1，其他叶子结点置为0，将GBDT所有子树的所有叶子结点的值合起来形成输入Logistic回归的特征向量。

---

<script type="text/javascript" src="/include/tail.js"></script>
