---
title: 《深度学习推荐系统》（第2章）阅读笔记
---

# 《深度学习推荐系统》（第2章）阅读笔记

<script type="text/javascript" src="/include/head.js"></script>

《深度学习推荐系统》，作者王喆，2020年3月第1版，第2章《前深度学习时代——推荐系统的进化之路》。

## 概述

传统推荐模型可以分为：

协同过滤类：物品协同过滤ItemCF、用户协同过滤UserCF、矩阵分解MF；

Logistic回归类：LR、大规模分片线性模型LS-PLM；

因子分解机FM类：POLY2、FM、域感知因子分解机FFM；

组合模型类：GBDT+LR。

## 一、协同过滤

协同用户们的评价一起对海量物品进行过滤，从中筛选出目标用户可能感兴趣的物品的推荐过程就叫协同过滤。

### 1、朴素协同过滤

现有共现矩阵（用户-物品矩阵），协同过滤有UserCF和ItemCF两种。

#### UserCF

由用户向量计算用户之间的相似度，朴素协同过滤的用户向量是用户-物品矩阵中该用户这一行的数据构成的向量；

对要推荐的用户得到与其最相近的topk个用户，然后利用用户相似度和相似用户对物品评价的加权平均来预测该用户对物品的评分。

#### ItemCF

由物品向量计算物品之间的相似度，朴素协同过滤的物品向量可以是用户-物品矩阵中该物品这一列的数据构成的向量；

找出与该物品相似的topk个正反馈物品，组成相似物品集合，然后利用物品相似度和用户对相似物品评价的加权平均来预测用户对该物品的评分。

#### UserCF和ItemCF的应用场景

UserCF的缺点：因为需要维护用户相似度矩阵，而用户的增长会导致存储开销过大；用户历史数据向量非常稀疏，找到的相似用户准确度低，不适用于正反馈较难获取的场景。

UserCF具备较强的社交特性，可以通过相似兴趣的人将以前不在自己兴趣范围内的热点更新到推荐列表中，适用于新闻推荐场景，擅长发现和跟踪热点。

ItemCF适用于兴趣变化较为稳定的应用，例如电商、影视推荐。

#### 朴素协同过滤的优缺点

优点：直观、可解释性强；

缺点：泛化能力弱，推荐结果的头部效应明显（处理稀疏向量的能力弱）（通过矩阵分解解决）；无法引入用户特征、物品特征、上下文特征（通过Logistic回归解决）。

### 2、矩阵分解协同过滤

引入隐向量表示用户和物品向量，即将共现矩阵$R_{m \times n}$分解为用户矩阵$U_{m \times k}$和物品矩阵$V_{k \times n}$相乘的形式，其中，$m$是用户数量，$n$是物品数量，$k$是隐向量维度。

$k$越小，隐向量表达能力越弱，模型泛化程度越高；反之反是；$k$的取值是推荐效果和工程开销的平衡点。

用户$u$对物品$i$的评分为对应用户隐向量和物品隐向量的内积。

#### 梯度下降矩阵分解

一般采用梯度下降法进行矩阵分解：

$$Loss = \Sigma_{(u, i) \in K} (r_{ui} - q_i^Tp_u) ^ 2$$

其中，$K$为所有用户评分样本集合，$r_{ui}$为用户$u$对物品$i$评分，$q_i$为物品$i$隐向量，$p_u$为用户$u$隐向量。

为了减少过拟合，可以加入正则化项：

$$Loss = \Sigma_{(u, i) \in K} [(r_{ui} - q_i^Tp_u) ^ 2 + \lambda (\left \| q_i \right \| ^ 2 + \left \| p_u \right \| ^ 2)]$$

然后对损失函数求偏导，按照学习率更新隐向量$q_i$和$p_u$，直到迭代次数达到上限或者损失低于阈值。

#### 消除打分偏差

引入全局偏差常数$\mu$、物品偏差参数$b_i$（可用物品收到的所有评分均值）、用户偏差参数$b_u$（可用用户给出的所有评分均值）：

$$Loss = \Sigma_{(u, i) \in K} [(r_{ui} - \mu - b_u - b_i - q_i^Tp_u) ^ 2 + \lambda (\left \| q_i \right \| ^ 2 + \left \| p_u \right \| ^ 2 + b_u^2 + b_i^2)]$$

然后求偏导，更新隐向量$q_i$、$p_u$和参数$b_u$、$b_i$。

#### 矩阵分解协同过滤优缺点

优点：泛化能力强，一定程度解决数据稀疏问题；隐向量维数低，空间复杂度低；扩展性更好，矩阵分解结果可以和其他特征组合拼接，与DNN无缝衔接。

缺点：无法引入用户特征、物品特征、上下文特征。

## 二、Logistic回归

Logistic回归输出正样本的概率（比如点击率CTR），用正样本的概率排序，将成为正样本概率最高的物品推荐给用户；输入是用户的若干特征组成的特征向量。

### 特征交叉自动化

原始的Logistic回归特征之间是不交叉的，但事实上各个特征并不是独立的，所以需要特征交叉。

#### POLY2

POLY2本质上就是二阶多项式的Logistic回归，将Logistic回归中的线性函数替换成二阶函数，在套上Logistic函数。

#### FM

所有二阶项的系数矩阵是一个正定的对称矩阵（$W_{n\times n}$），可以将其分解（$W_{n\times n} = M_{n\times k}M_{n\times k}^T$），也就是说，交叉项前面的系数原本是一个常数，现在改成了两个向量的内积，这两个向量分别是交叉项两个子项的特征隐向量。

#### FFM

每个项为所有项（包括自己）创建一个特征隐向量，也就是每个项不再是一个特征隐向量，而是一组特征隐向量，当$x_1$与$x_2$交叉时，前面是系数是$x_1$特征隐向量组中与$x_2$对应的特征隐向量与$x_2$特征隐向量组中与$x_1$对应的特征隐向量的内积。

### 特征工程模型化

用GBDT生成特征向量：将训练样本输入GBDT的一个子树后，会落入一个叶子结点中，将该叶子结点置为1，其他叶子结点置为0，将GBDT所有子树的所有叶子结点的值合起来形成输入Logistic回归的特征向量。

---

<script type="text/javascript" src="/include/tail.js"></script>
