---
title: 论文《ESMM: An Effective Approach for Estimating Post-Click Conversion Rate》阅读
---

# 论文《ESMM: An Effective Approach for Estimating Post-Click Conversion Rate》阅读

<script type="text/javascript" src="/include/head.js"></script>

本文提出了ESMM模型（全称Entire Space Multi-task Model），为CVR的建模提供了新范式。

## CVR建模及其挑战

用户行为序列是从曝光impression、到点击click、再到转化conversion，CVR建模的是点击后的转化概率，也就是：

$$pCVR = p(conversion|click,impression)$$

传统的CVR模型存在下面两个问题：

### Sample Selection Bias 样本选择偏差

使用传统CVR模型进行推断时，数据集样本来自于所有曝光impression样本组成的数据集，但是训练CVR模型时，使用的数据集样本只来自有点击click的曝光impression样本。

事实上，推断样本空间是全空间，而训练样本空间只是其子集。往往推断样本空间这个子集的分布与训练样本空间这个全集的分布不一致，从而影响CVR模型的泛化性能。

### Data Sparsity 数据稀疏

传统CVR模型只使用有点击click的曝光impression样本训练，使得训练样本的数量比CTR模型要少很多，CVR模型的拟合会比较困难。

## ESMM模型

ESMM引入CTR和CTCVR两个辅助任务来解决上面两个问题。对于一个给定的曝光impression样本，ESMM同时输出pCTR、pCVR和pCTCVR。ESMM主要由CTR和CVR两个子网络中组成，两个子网络都是Embedding&MLP的结构，CTCVR将CVR和CTR两个子网络的输出乘积作为输出。

### 在全空间建模

pCTR、pCVR和pCTCVR三者的关系可以表示为：

$$pCTCVR = pCTR \times pCVR$$

给定曝光impression样本$x$，上式等价于：

$$p(y=1,z=1|x) = p(y=1|x) \times p(z=1|y=1,x)$$

那么有：

$$p(z=1|y=1,x) = \frac{p(y=1,z=1|x)}{p(y=1|x)}$$

这里的pCTR和pCTCVR两个概率都是在全样本空间上训练的，通过估计pCTCVR和pCTR，可以在全样本空间上推导出pCVR，这直接解决了样本选择偏差问题。

不过，ESMM并不是通过单独训练CTR和CTCVR两个模型，然后将pCTCVR与pCTR相除这种除法方案来获得pCVR的。因为pCTR是一个很小的数值，作分母会导致数值不稳定。所以ESMM采用乘法方案来避免数值不稳定。

在ESMM中，pCVR只是一个受到上面乘法公式约束的中间变量，ESMM在全样本空间上估计的还是pCTR和pCTCVR，乘法使三个相互关联、共同训练的估计器能够利用行为序列的顺序关系，并在训练期间相互共享信息。另外，乘法方案确保估计的pCVR值在`[0,1]`范围内，而除法方案中pCVR可能大于`1`。

ESMM的损失函数为：

$$L(\theta_{cvr}, \theta_{ctr}) = \sum_{i=1}^{N}l(y_i,f(x_i;\theta_{ctr}))+\sum_{i=1}^{N}l(y_i\&z_i,f(x_i;\theta_{ctr})\times f(x_i;\theta_{cvr}))$$

其中，其中$θ_{ctr}$和$θ_{cvr}$ 是CTR和CVR网络的参数，$l(·)$是交叉熵损失函数。

### 特征表示迁移

CVR和CTR两个子网络都有Embedding层，它贡献了网络的大部分参数，需要大量训练样本来学习。在ESMM中，CVR网络的Embedding与CTR网络的Embedding共享，遵循特征表示迁移学习范式。CTR任务使用所有曝光impression的训练样本，对比传统CVR任务只使用有点击click的曝光impression样本来说样本数量要丰富得多。这种参数共享机制使ESMM中的CVR网络能够从未点击click的曝光impression样本中学习，缓解数据稀疏问题。

---

<script type="text/javascript" src="/include/tail.js"></script>