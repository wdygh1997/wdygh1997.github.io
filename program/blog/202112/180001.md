---
title: 论文《DeepFM: A Factorization-Machine based Neural Network for CTR Prediction》阅读
---

# 论文《DeepFM: A Factorization-Machine based Neural Network for CTR Prediction》阅读

<script type="text/javascript" src="/include/head.js"></script>

本文提出了DeepFM模型，一种结合了因子分解机和深度神经网络的端到端模型，减少了对人工特征工程的依赖。

## 建模特征交互

对于CTR预估，重要的一点在于学习特征之间的交互。现有的模型要么偏重于低阶特征交互，要么偏重于高阶特征交互，要么依赖人工特征工程。

本文提出的DeepFM，联合训练因子分解机和深度神经网络两个部分，具有下列优势：

1、不需要任何预训练；

2、同时学习高阶（通过深度神经网络）和低阶（通过因子分解机）的特征交互；

3、以端到端的方式学习，共享特征Embedding，避免了人工特征工程，并且减少了参数量。

## DeepFM模型

输入是分类特征的one-hot编码、连续特征的原值或者离散化后的one-hot编码，这些特征通常来自于一个用户&商品对。

DeepFM包括FM和DNN两个部分：

$$\hat{y} = \sigma(y_{FM}+y_{DNN})$$

FM和DNN共享相同的特征Embedding。

### 因子分解机FM部分

在FM中，对于特征$x_i$，用标量$w_i$来衡量它的一阶重要性，用向量$V_i$来衡量它与其他特征的二阶交互的影响（建模为两个$V$向量的内积）。

以往只有当特征$i$和特征$j$出现在同一条训练记录中，才能训练特征$i$和$j$的二阶交互参数。而在FM中，二阶交互是通过向量$V_i$和$V_j$的内积来衡量，$i$单独出现在数据记录中，也可以训练向量$V_i$。因此，FM可以学习在训练数据中从未出现或很少出现的特征交互。

$$y_{FM} = \langle w, x \rangle + \sum_{i = 1}^d\sum_{j = i+1}^d \langle V_i,V_j \rangle x_i  x_j$$

其中，$w \in R^d$，$x \in R^d$，$V_i \in R^k$，$d$是输入特征维度，$k$是Embedding向量维度。

上述公式，前半部分代表一阶特征的重要性，后半部分代表二阶特征交互。

### 深度神经网络DNN部分

DNN学习特征的高阶交互，输入特征稀疏高维，在进入神经网络之前，需要Embedding层将输入压缩为低维稠密向量。Embedding层的参数在模型训练过程中被学习，Embedding向量维度固定为$k$。

神经网络每层运算如下：

$$a^{(l+1)} = \sigma (W^{(l)} a^{(l)} + b^{(l)})$$

其中，$\sigma$是激活函数，$a^{(l)}$，$W^{(l)}$，$b^{(l)}$是第$l$层的输入、权重、偏置。

最后送入sigmoid函数进行CTR预测：

$$y_{DNN} = W^{H+1} a^{H} + b^{H+1}$$

其中，$H$是隐藏层的数量。

---

<script type="text/javascript" src="/include/tail.js"></script>
